---
title: "Twitter Daily Analysis – `r params$report_date`"
author: ""
date: "`r Sys.Date()`"
params:
  report_date: !r Sys.Date()   # change for back‑reporting
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: false
  pdf_document:
    latex_engine: xelatex
    toc: true
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# ── Packages ──────────────────────────────────────────────────────────────
required <- c(
  "tidyverse", "lubridate", "tidytext", "stringi",
  "knitr", "kableExtra", "sentimentr",
  "DBI", "RPostgres"
)
invisible(lapply(required, \(p){
  if (!requireNamespace(p, quietly = TRUE)) install.packages(p, quiet = TRUE)
  library(p, character.only = TRUE)
}))

safe_kable <- function(tbl, digits = 1, caption = NULL){
  if (nrow(tbl) == 0){
    knitr::kable(data.frame(Note = "No data for the selected day"),
                 align = "c", caption = caption)
  } else {
    tbl |>
      kbl(digits = digits, align = "c", caption = caption) |>
      kable_styling(
        bootstrap_options = c("striped", "hover", "condensed"),
        full_width = FALSE, position = "center"
      ) |>
      row_spec(0, bold = TRUE, color = "white", background = "#000")
  }
}
```



```{r, echo = FALSE,warning = FALSE,message = FALSE}


# --- Supabase creds (replace with secrets in production) ----
Sys.setenv(
  SUPABASE_HOST = "aws-0-us-east-2.pooler.supabase.com",
  SUPABASE_PORT = "6543",
  SUPABASE_DB   = "postgres",
  SUPABASE_USER = "postgres.kubvrwnqmsmhwcuscvje",
  SUPABASE_PWD  = "hfa-tgt8nkj1AVM9vqe"
)

con <- DBI::dbConnect(
  RPostgres::Postgres(),
  host     = Sys.getenv("SUPABASE_HOST"),
  port     = as.integer(Sys.getenv("SUPABASE_PORT")),
  dbname   = Sys.getenv("SUPABASE_DB"),
  user     = Sys.getenv("SUPABASE_USER"),
  password = Sys.getenv("SUPABASE_PWD"),
  sslmode  = "require"
)

twitter_raw <- DBI::dbReadTable(con, "twitter_raw")

main_ids <- tibble::tribble(
  ~username,            ~main_id,
  "weave_db",           "1206153294680403968",
  "OdyseeTeam",         "1280241715987660801",
  "ardriveapp",         "1293193263579635712",
  "redstone_defi",      "1294053547630362630",
  "everpay_io",         "1334504432973848577",
  "decentlandlabs",     "1352388512788656136",
  "KYVENetwork",        "136377177683878784",
  "onlyarweave",        "1393171138436534272",
  "ar_io_network",      "1468980765211955205",
  "Permaswap",          "1496714415231717380",
  "communitylabs",      "1548502833401516032",
  "usewander",          "1559946771115163651",
  "apus_network",       "1569621659468054528",
  "fwdresearch",        "1573616135651545088",
  "perma_dao",          "1595075970309857280",
  "Copus_io",           "1610731228130312194",
  "basejumpxyz",        "1612781645588742145",
  "AnyoneFDN",          "1626376419268784130",
  "arweaveindia",       "1670147900033343489",
  "useload",            "1734941279379759105",
  "protocolland",       "1737805485326401536",
  "aoTheComputer",      "1750584639385939968",
  "ArweaveOasis",       "1750723327315030016",
  "aox_xyz",            "1751903735318720512",
  "astrousd",           "1761104764899606528",
  "PerplexFi",          "1775862139980226560",
  "autonomous_af",      "1777500373378322432",
  "Liquid_Ops",         "1795772412396507136",
  "ar_aostore",         "1797632049202794496",
  "FusionFiPro",        "1865790600462921728",
  "vela_ventures",      "1869466343000444928",
  "beaconwallet",       "1879152602681585664",
  "VentoSwap",          "1889714966321893376",
  "permawebjournal",    "1901592191065300993",
  "Botega_AF",          "1902521779161292800",
  "samecwilliams",      "409642632",
  "TateBerenbaum",      "801518825690824707",
  "ArweaveEco",         "892752981736779776"
)

tweets_tagged <- twitter_raw |>
  left_join(main_ids, by = "username") |>
  mutate(
    is_rt_text = str_detect(text, "^RT @"),
    post_type = case_when(
      is_rt_text                                      ~ "retweet",
      user_id == main_id & !is_rt_text &
        str_detect(text, "https://t.co")              ~ "quote",
      user_id == main_id                              ~ "original",
      TRUE                                            ~ "other"
    )
  )

# ── 1  Define the rolling window ------------------------------------------
window_end   <- lubridate::now(tzone = "UTC")       # adjust tz if needed
window_start <- window_end - lubridate::dhours(24)

# If you still want to anchor the window to a param value, replace the two
# lines above with something like:
#   window_end   <- as.POSIXct(params$report_end,   tz = "UTC")
#   window_start <- window_end - lubridate::dhours(24)

# ── 2  Filter tweets --------------------------------------------------------
df_day <- tweets_tagged |>
  mutate(
    publish_dt = lubridate::ymd_hms(date, tz = "UTC"),   # ensure POSIXct
    hour       = lubridate::hour(publish_dt),
    weekday    = lubridate::wday(publish_dt,
                                 label = TRUE, abbr = FALSE, locale = "en_US")
  ) |>
  filter(
    publish_dt >= window_start,
    publish_dt <= window_end,
    post_type  != "other"
  )

# ── 3  Early‑exit guard -----------------------------------------------------
if (nrow(df_day) == 0){
  cat(
    "\n\n### No tweets in the last 24 hours (",
    format(window_start, "%Y‑%m‑%d %H:%M"), " → ",
    format(window_end,   "%Y‑%m‑%d %H:%M"), ").\n\n"
  )
  knitr::knit_exit()
}

```


# Summary Table
```{r}

summary_table <- df_day |>
  summarise(
    total_tweets    = n(),
    avg_likes       = mean(like_count,  na.rm = TRUE),
    avg_comments    = mean(reply_count, na.rm = TRUE),
    avg_impressions = mean(view_count,  na.rm = TRUE),
    avg_engagement  = mean(engagement_rate, na.rm = TRUE)
  )
safe_kable(summary_table)

```

# Top Keywords (`r params$report_date`)

```{r}
custom_stop <- tibble(word = c("ao","aothecomputer","rt","https","t.co","1"))
word_counts <- df_day |>
  unnest_tokens(word, text) |>
  anti_join(bind_rows(stop_words, custom_stop), by = "word") |>
  count(word, sort = TRUE) |>
  slice_head(n = 20)

if (nrow(word_counts) > 0){
  ggplot(word_counts, aes(reorder(word, n), n)) +
    geom_col(fill = "steelblue") +
    coord_flip() +
    labs(title = "Top 20 Words", x = "Word", y = "Frequency") +
    theme_minimal()
} else {
  cat("*No tokens available for this day.*")
}
```

# TF-IDF by Post Type (`r params$report_date`)

```{r}
word_tfidf <- df_day |>
  mutate(text = str_remove_all(text, "http\\S+|@\\w+|[[:punct:]]")) |>
  unnest_tokens(word, text) |>
  anti_join(stop_words, by = "word") |>
  filter(!str_detect(word,"^[0-9]+$"), word != "rt") |>
  count(post_type, word, sort = TRUE) |>
  bind_tf_idf(word, post_type, n) |>
  group_by(post_type) |>
  slice_max(tf_idf, n = 10) |>
  ungroup()

if (nrow(word_tfidf) > 0){
  ggplot(word_tfidf,
         aes(reorder_within(word, tf_idf, post_type), tf_idf, fill = post_type)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~post_type, scales = "free_y") +
    scale_x_reordered() +
    labs(title = "Distinctive Words by Post Type",
         x = "Word", y = "TF‑IDF") +
    coord_flip() +
    theme_minimal()
} else {
  cat("*Insufficient tokens to compute TF‑IDF.*")
}

```

# Time-Based Analysis (`r params$report_date`)
```{r}

hourly_dist <- df_day |>
  count(hour) |>
  mutate(perc = n/sum(n)*100)

if (nrow(hourly_dist) > 0){
  ggplot(hourly_dist, aes(hour, perc)) +
    geom_col(fill = "darkorange") +
    scale_x_continuous(breaks = 0:23) +
    labs(title = "Tweet Activity by Hour",
         x = "Hour", y = "% of Tweets") +
    theme_minimal()
} else cat("*No hourly data.*")


```




# Engagement Analysis

```{r}
eng_by_hour <- df_day |>
  group_by(hour) |>
  summarise(mean_eng = mean(engagement_rate, na.rm = TRUE), .groups="drop")

if (nrow(eng_by_hour) > 0){
  ggplot(eng_by_hour, aes(hour, mean_eng)) +
    geom_line(color = "steelblue", size = 1) +
    geom_point(color = "darkblue", size = 2) +
    labs(title = "Average Engagement by Hour",
         x = "Hour", y = "Engagement Rate") +
    theme_minimal()
} else cat("*No engagement data.*")


```


```{r}

eng_by_type <- df_day %>%
  group_by(post_type) %>%
  summarise(avg_eng = mean(engagement_rate, na.rm = TRUE), .groups = "drop")

ggplot(eng_by_type, aes(post_type, avg_eng, fill = post_type)) +
  geom_col() +
  labs(title = "Average Engagement by Post Type",
       x = "Post Type", y = "Engagement Rate") +
  theme_minimal() +
  theme(legend.position = "none")


```




# Likes Analysis

```{r}
metric_plot <- function(metric, ylab){
  df <- df_day |>
    group_by(hour) |>
    summarise(avg_val = mean(.data[[metric]], na.rm = TRUE), .groups="drop")
  if (nrow(df)==0) {
    cat(paste("*No", ylab, "data.*"))
  } else {
    ggplot(df, aes(hour, avg_val)) +
      geom_line(color="steelblue", size=1) +
      geom_point(color="darkblue", size=2) +
      labs(title = paste("Average", ylab, "by Hour"),
           x="Hour", y=ylab) +
      theme_minimal()
  }
}

metric_plot("like_count", "Likes")


```

```{r}
## ── 1  Average likes ────────────────────────────────────────────────────
likes_by_type <- df_day %>%
  group_by(post_type) %>%
  summarise(avg_likes = mean(like_count, na.rm = TRUE), .groups = "drop")

ggplot(likes_by_type, aes(post_type, avg_likes, fill = post_type)) +
  geom_col() +
  labs(title = "Average Likes by Post Type",
       x = "Post Type", y = "Likes") +
  theme_minimal() +
  theme(legend.position = "none")
```



# Comments Analysis

```{r}
metric_plot("reply_count",  "Comments")

```

```{r}
## ── 2  Average comments ────────────────────────────────────────────────
comments_by_type <- df_day %>%
  group_by(post_type) %>%
  summarise(avg_comments = mean(reply_count, na.rm = TRUE), .groups = "drop")

ggplot(comments_by_type, aes(post_type, avg_comments, fill = post_type)) +
  geom_col() +
  labs(title = "Average Comments by Post Type",
       x = "Post Type", y = "Comments") +
  theme_minimal() +
  theme(legend.position = "none")
```


# Impressions Analysis

```{r}
metric_plot("view_count",   "Impressions")

```


```{r}
## ── 3  Average impressions ─────────────────────────────────────────────
views_by_type <- df_day %>%
  group_by(post_type) %>%
  summarise(avg_views = mean(view_count, na.rm = TRUE), .groups = "drop")

ggplot(views_by_type, aes(post_type, avg_views, fill = post_type)) +
  geom_col() +
  labs(title = "Average Impressions by Post Type",
       x = "Post Type", y = "Impressions") +
  theme_minimal() +
  theme(legend.position = "none")
```


# Hashtag performance

```{r}
hashtags <- df_day |>
  mutate(tag = str_extract_all(text, "#\\w+")) |>
  unnest(tag) |>
  group_by(tag) |>
  summarise(
    avg_eng = mean(engagement_rate, na.rm = TRUE),
    uses    = n(), .groups="drop") |>
  filter(uses >= 3) |>
  arrange(desc(avg_eng)) |>
  slice_head(n=10)

if (nrow(hashtags)>0){
  ggplot(hashtags, aes(reorder(tag, avg_eng), avg_eng)) +
    geom_col(fill = "purple") +
    coord_flip() +
    labs(title = "Top Hashtags by Engagement",
         x="Hashtag", y="Avg Engagement Rate") +
    theme_minimal()
} else cat("*No hashtag meets frequency threshold.*")

```
